"""
T-Max Ultimate - Evaluator Agent

Best-of-Nè‡ªå‹•æ¡ç‚¹ã‚·ã‚¹ãƒ†ãƒ :
- è¤‡æ•°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æˆæœç‰©ã‚’è‡ªå‹•è©•ä¾¡
- æ©Ÿæ¢°çš„ãƒ»å†ç¾çš„ãªæ¡ç‚¹
- å‹è€…è‡ªå‹•æ±ºå®š
- å­¦ç¿’ãƒ‡ãƒ¼ã‚¿è“„ç©
"""
import subprocess
import json
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from datetime import datetime
import sys
import hashlib

sys.path.append(str(Path(__file__).parent.parent.parent))

from tmax_work3.blackboard.state_manager import (
    Blackboard,
    AgentType,
    TaskStatus,
    get_blackboard
)


class EvaluatorAgent:
    """
    Evaluator Agent - Best-of-Nè‡ªå‹•æ¡ç‚¹

    æ©Ÿèƒ½:
    - pytestå®Ÿè¡Œçµæœã®è©•ä¾¡
    - å·®åˆ†ã®è¤‡é›‘åº¦åˆ†æ
    - ã‚³ãƒ¼ãƒ‰å“è³ªãƒã‚§ãƒƒã‚¯ï¼ˆPylint, Banditï¼‰
    - ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯
    - ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—ã¨å‹è€…æ±ºå®š
    """

    def __init__(self, repository_path: str):
        self.repo_path = Path(repository_path)
        self.blackboard = get_blackboard()

        # è©•ä¾¡ã‚¦ã‚§ã‚¤ãƒˆï¼ˆã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ï¼‰
        self.weights = {
            "test_pass_rate": 0.5,  # ãƒ†ã‚¹ãƒˆåˆæ ¼ç‡ï¼ˆæœ€é‡è¦ï¼‰
            "diff_complexity": 0.2,  # å·®åˆ†ã®è¤‡é›‘åº¦ï¼ˆä½ã„ã»ã©è‰¯ã„ï¼‰
            "code_quality": 0.2,     # ã‚³ãƒ¼ãƒ‰å“è³ªã‚¹ã‚³ã‚¢
            "doc_consistency": 0.1   # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸€è²«æ€§
        }

        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç™»éŒ²
        try:
            self.blackboard.register_agent(
                AgentType.QA,  # æ—¢å­˜ã®QAã‚¿ã‚¤ãƒ—ã‚’ä½¿ç”¨ï¼ˆå¾Œã§EVALUATORã‚’è¿½åŠ ï¼‰
                worktree="main"
            )
        except:
            pass  # æ—¢ã«ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—

        self.blackboard.log(
            "ğŸ† Evaluator Agent initialized - Best-of-N scoring system ready",
            level="INFO",
            agent=AgentType.QA
        )

    def evaluate_candidates(self, candidates: List[Dict]) -> Dict:
        """
        è¤‡æ•°å€™è£œã‚’è©•ä¾¡ã—ã€å‹è€…ã‚’æ±ºå®š

        Args:
            candidates: å€™è£œãƒªã‚¹ãƒˆ
                [
                    {
                        "id": "agent-01",
                        "worktree_path": "/path/to/worktree",
                        "branch": "parallel/agent-01"
                    },
                    ...
                ]

        Returns:
            è©•ä¾¡çµæœ
                {
                    "evaluated_at": "2025-11-05T20:00:00Z",
                    "candidates": [
                        {
                            "id": "agent-01",
                            "metrics": {...},
                            "score": 0.85
                        },
                        ...
                    ],
                    "winner": "agent-01",
                    "decision_rule": "weighted_sum"
                }
        """
        self.blackboard.log(
            f"ğŸ” Evaluating {len(candidates)} candidates...",
            level="INFO",
            agent=AgentType.QA
        )

        scores = []

        for candidate in candidates:
            candidate_id = candidate["id"]
            worktree_path = Path(candidate["worktree_path"])

            self.blackboard.log(
                f"ğŸ“Š Evaluating candidate: {candidate_id}",
                level="INFO",
                agent=AgentType.QA
            )

            # 1. ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            test_result = self._run_tests(worktree_path)

            # 2. å·®åˆ†åˆ†æ
            diff_stats = self._analyze_diff(worktree_path)

            # 3. ã‚³ãƒ¼ãƒ‰å“è³ªãƒã‚§ãƒƒã‚¯
            quality_score = self._check_code_quality(worktree_path)

            # 4. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯
            doc_score = self._check_documentation(worktree_path)

            # 5. ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
            final_score = (
                self.weights["test_pass_rate"] * test_result["pass_rate"] +
                self.weights["diff_complexity"] * (1 - diff_stats["complexity_norm"]) +
                self.weights["code_quality"] * quality_score +
                self.weights["doc_consistency"] * doc_score
            )

            scores.append({
                "id": candidate_id,
                "metrics": {
                    "test_pass_rate": test_result["pass_rate"],
                    "test_passed": test_result["passed"],
                    "test_failed": test_result["failed"],
                    "diff_lines": diff_stats["total_lines"],
                    "complexity": diff_stats["complexity"],
                    "quality_score": quality_score,
                    "doc_score": doc_score
                },
                "score": round(final_score, 4)
            })

            self.blackboard.log(
                f"âœ… {candidate_id} score: {final_score:.4f}",
                level="INFO",
                agent=AgentType.QA
            )

        # å‹è€…æ±ºå®š
        winner = max(scores, key=lambda x: x["score"])

        result = {
            "evaluated_at": datetime.now().isoformat(),
            "candidates": scores,
            "winner": winner["id"],
            "winner_score": winner["score"],
            "decision_rule": self._format_decision_rule(),
            "weights": self.weights
        }

        self.blackboard.log(
            f"ğŸ¥‡ Winner: {winner['id']} with score {winner['score']:.4f}",
            level="SUCCESS",
            agent=AgentType.QA
        )

        # Blackboardã«è¨˜éŒ²
        self._record_evaluation(result)

        return result

    def _run_tests(self, worktree_path: Path) -> Dict:
        """
        pytestã‚’å®Ÿè¡Œã—ã¦ãƒ†ã‚¹ãƒˆçµæœã‚’å–å¾—

        Returns:
            {
                "passed": 10,
                "failed": 2,
                "total": 12,
                "pass_rate": 0.833
            }
        """
        try:
            result = subprocess.run(
                ["pytest", "--tb=short", "-v", "--json-report", "--json-report-file=test_report.json"],
                cwd=worktree_path,
                capture_output=True,
                text=True,
                timeout=300
            )

            # JSONãƒ¬ãƒãƒ¼ãƒˆèª­ã¿è¾¼ã¿
            report_file = worktree_path / "test_report.json"
            if report_file.exists():
                report = json.loads(report_file.read_text())

                passed = report["summary"]["passed"]
                failed = report["summary"]["failed"]
                total = report["summary"]["total"]

                return {
                    "passed": passed,
                    "failed": failed,
                    "total": total,
                    "pass_rate": passed / total if total > 0 else 0.0
                }
        except Exception as e:
            self.blackboard.log(
                f"âš ï¸ Test execution failed: {e}",
                level="WARNING",
                agent=AgentType.QA
            )

        # ãƒ†ã‚¹ãƒˆå¤±æ•—æ™‚ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        return {
            "passed": 0,
            "failed": 0,
            "total": 0,
            "pass_rate": 0.0
        }

    def _analyze_diff(self, worktree_path: Path) -> Dict:
        """
        git diffã‚’åˆ†æã—ã¦å·®åˆ†ã®è¤‡é›‘åº¦ã‚’è©•ä¾¡

        Returns:
            {
                "total_lines": 120,
                "added": 80,
                "deleted": 40,
                "complexity": 3.5,
                "complexity_norm": 0.35
            }
        """
        try:
            result = subprocess.run(
                ["git", "diff", "--stat", "HEAD~1..HEAD"],
                cwd=worktree_path,
                capture_output=True,
                text=True
            )

            lines = result.stdout.strip().split("\n")
            if not lines:
                return self._default_diff_stats()

            # æœ€çµ‚è¡Œã‹ã‚‰çµ±è¨ˆæƒ…å ±ã‚’æŠ½å‡º
            summary = lines[-1]
            # ä¾‹: "10 files changed, 120 insertions(+), 40 deletions(-)"

            added = 0
            deleted = 0

            if "insertion" in summary:
                added = int(summary.split("insertion")[0].split(",")[-1].strip())
            if "deletion" in summary:
                deleted = int(summary.split("deletion")[0].split(",")[-1].strip())

            total_lines = added + deleted

            # è¤‡é›‘åº¦è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆ: å¤‰æ›´è¡Œæ•°ã®å¯¾æ•°ï¼‰
            import math
            complexity = math.log10(max(total_lines, 1) + 1)
            complexity_norm = min(complexity / 10.0, 1.0)  # 0-1ã«æ­£è¦åŒ–

            return {
                "total_lines": total_lines,
                "added": added,
                "deleted": deleted,
                "complexity": round(complexity, 2),
                "complexity_norm": round(complexity_norm, 4)
            }
        except Exception as e:
            self.blackboard.log(
                f"âš ï¸ Diff analysis failed: {e}",
                level="WARNING",
                agent=AgentType.QA
            )
            return self._default_diff_stats()

    def _default_diff_stats(self) -> Dict:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å·®åˆ†çµ±è¨ˆ"""
        return {
            "total_lines": 0,
            "added": 0,
            "deleted": 0,
            "complexity": 0.0,
            "complexity_norm": 0.0
        }

    def _check_code_quality(self, worktree_path: Path) -> float:
        """
        ã‚³ãƒ¼ãƒ‰å“è³ªã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆPylint, Banditï¼‰

        Returns:
            å“è³ªã‚¹ã‚³ã‚¢ï¼ˆ0-1ï¼‰
        """
        scores = []

        # Pylint
        try:
            result = subprocess.run(
                ["pylint", "--output-format=json", "app/", "tmax_work3/"],
                cwd=worktree_path,
                capture_output=True,
                text=True,
                timeout=60
            )

            # Pylintã‚¹ã‚³ã‚¢ã‚’è§£æï¼ˆ0-10 â†’ 0-1ï¼‰
            # ç°¡æ˜“ç‰ˆ: ã‚¨ãƒ©ãƒ¼ãŒãªã‘ã‚Œã°0.8ã€ã‚ã‚Œã°0.5
            if result.returncode == 0:
                scores.append(0.8)
            else:
                scores.append(0.5)
        except:
            scores.append(0.5)

        # Banditï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯ï¼‰
        try:
            result = subprocess.run(
                ["bandit", "-r", "app/", "-f", "json"],
                cwd=worktree_path,
                capture_output=True,
                text=True,
                timeout=60
            )

            # Banditçµæœã‚’è§£æ
            if result.returncode == 0:
                scores.append(0.9)
            else:
                scores.append(0.6)
        except:
            scores.append(0.6)

        # å¹³å‡ã‚¹ã‚³ã‚¢
        return sum(scores) / len(scores) if scores else 0.5

    def _check_documentation(self, worktree_path: Path) -> float:
        """
        ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸€è²«æ€§ã‚’ãƒã‚§ãƒƒã‚¯

        - README.mdã®å­˜åœ¨
        - docstringã®å­˜åœ¨ç‡
        - CHANGELOG.mdã®æ›´æ–°

        Returns:
            ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢ï¼ˆ0-1ï¼‰
        """
        score = 0.0

        # README.mdå­˜åœ¨ãƒã‚§ãƒƒã‚¯
        readme = worktree_path / "README.md"
        if readme.exists() and readme.stat().st_size > 100:
            score += 0.4

        # CHANGELOG.mdæ›´æ–°ãƒã‚§ãƒƒã‚¯
        changelog = worktree_path / "CHANGELOG.md"
        if changelog.exists():
            score += 0.3

        # Pythonãƒ•ã‚¡ã‚¤ãƒ«ã®docstringå­˜åœ¨ç‡ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        python_files = list(worktree_path.glob("**/*.py"))
        if python_files:
            docstring_count = 0
            for py_file in python_files[:10]:  # æœ€åˆã®10ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                try:
                    content = py_file.read_text()
                    if '"""' in content or "'''" in content:
                        docstring_count += 1
                except:
                    pass

            docstring_rate = docstring_count / min(len(python_files), 10)
            score += 0.3 * docstring_rate

        return round(score, 4)

    def _format_decision_rule(self) -> str:
        """æ±ºå®šãƒ«ãƒ¼ãƒ«ã‚’æ–‡å­—åˆ—åŒ–"""
        return (
            f"{self.weights['test_pass_rate']}*test_pass + "
            f"{self.weights['diff_complexity']}*(1-diff_norm) + "
            f"{self.weights['code_quality']}*quality + "
            f"{self.weights['doc_consistency']}*doc"
        )

    def _record_evaluation(self, result: Dict):
        """è©•ä¾¡çµæœã‚’Blackboardã«è¨˜éŒ²"""
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        eval_dir = self.repo_path / "tmax_work3" / "data" / "evaluations"
        eval_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        eval_file = eval_dir / f"evaluation_{timestamp}.json"

        eval_file.write_text(json.dumps(result, indent=2))

        self.blackboard.log(
            f"ğŸ’¾ Evaluation saved: {eval_file}",
            level="INFO",
            agent=AgentType.QA
        )


# ãƒ†ã‚¹ãƒˆç”¨ã‚³ãƒ¼ãƒ‰
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="T-Max Evaluator Agent")
    parser.add_argument("--test", action="store_true", help="Run test")
    args = parser.parse_args()

    if args.test:
        print("ğŸ§ª Testing Evaluator Agent...")

        evaluator = EvaluatorAgent(".")

        # ãƒ†ã‚¹ãƒˆå€™è£œ
        candidates = [
            {
                "id": "test-agent-01",
                "worktree_path": ".",
                "branch": "main"
            }
        ]

        # è©•ä¾¡å®Ÿè¡Œ
        result = evaluator.evaluate_candidates(candidates)

        print("\nğŸ“Š Evaluation Result:")
        print(json.dumps(result, indent=2))

        print("\nâœ… Test complete!")
    else:
        print("Usage: python evaluator.py --test")
