"""
T-Max Work3 MLOps Agent
æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ç®¡ç†ãƒ»ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è‡ªå‹•åŒ–ãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤ã‚’æ‹…å½“

æ©Ÿèƒ½:
- ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è‡ªå‹•åŒ–
- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
- A/Bãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
- ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°
- MLflowçµ±åˆ
"""
import os
import subprocess
import json
import pickle
import hashlib
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime
import sys
import shutil

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent.parent))

from tmax_work3.blackboard.state_manager import (
    Blackboard,
    AgentType,
    TaskStatus,
    get_blackboard
)

try:
    import mlflow
    MLFLOW_AVAILABLE = True
except ImportError:
    MLFLOW_AVAILABLE = False


class MLOpsAgent:
    """
    MLOps Agent - æ©Ÿæ¢°å­¦ç¿’é‹ç”¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

    å½¹å‰²:
    - ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç®¡ç†
    - ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
    - ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã¨ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°
    - A/Bãƒ†ã‚¹ãƒˆã¨ã‚«ãƒŠãƒªã‚¢ãƒ‡ãƒ—ãƒ­ã‚¤
    - ãƒ¢ãƒ‡ãƒ«ç›£è¦–ã¨ãƒ‰ãƒªãƒ•ãƒˆæ¤œçŸ¥
    """

    def __init__(self, repository_path: str):
        self.repo_path = Path(repository_path)
        self.blackboard = get_blackboard()
        self.models_dir = self.repo_path / "models"
        self.experiments_dir = self.repo_path / "experiments"
        self.data_dir = self.repo_path / "data"
        self.reports_dir = self.repo_path / "tmax_work3" / "data" / "mlops_reports"

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.models_dir.mkdir(parents=True, exist_ok=True)
        self.experiments_dir.mkdir(parents=True, exist_ok=True)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.reports_dir.mkdir(parents=True, exist_ok=True)

        # MLflowè¨­å®š
        self.mlflow_tracking_uri = os.getenv("MLFLOW_TRACKING_URI", "file:./mlruns")
        if MLFLOW_AVAILABLE:
            mlflow.set_tracking_uri(self.mlflow_tracking_uri)

        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç™»éŒ²
        self.blackboard.register_agent(
            AgentType.MLOPS,
            worktree="main"
        )

        self.blackboard.log(
            f"ğŸ¤– MLOps Agent initialized (MLflow: {MLFLOW_AVAILABLE})",
            level="INFO",
            agent=AgentType.MLOPS
        )

    def train_model(
        self,
        model_name: str,
        training_script: str,
        params: Optional[Dict] = None,
        dataset_path: Optional[str] = None
    ) -> Tuple[bool, Dict]:
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°

        Args:
            model_name: ãƒ¢ãƒ‡ãƒ«å
            training_script: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ‘ã‚¹
            params: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            dataset_path: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ‘ã‚¹

        Returns:
            (success, training_report)
        """
        self.blackboard.log(
            f"ğŸ‹ï¸ Training model: {model_name}",
            level="INFO",
            agent=AgentType.MLOPS
        )

        if params is None:
            params = {}

        training_report = {
            "model_name": model_name,
            "started_at": datetime.now().isoformat(),
            "params": params,
            "success": False
        }

        try:
            # MLflowãƒ©ãƒ³ã‚’é–‹å§‹
            if MLFLOW_AVAILABLE:
                with mlflow.start_run(run_name=model_name):
                    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ­ã‚°
                    mlflow.log_params(params)

                    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
                    result = self._execute_training(training_script, params, dataset_path)

                    if result["success"]:
                        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ã‚°
                        mlflow.log_metrics(result.get("metrics", {}))

                        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
                        model_path = result.get("model_path")
                        if model_path and Path(model_path).exists():
                            mlflow.log_artifact(model_path)

                        training_report.update(result)
                        training_report["mlflow_run_id"] = mlflow.active_run().info.run_id

            else:
                # MLflowãªã—ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
                result = self._execute_training(training_script, params, dataset_path)
                training_report.update(result)

            training_report["completed_at"] = datetime.now().isoformat()

            if training_report["success"]:
                self.blackboard.log(
                    f"âœ… Model trained successfully: {model_name}",
                    level="SUCCESS",
                    agent=AgentType.MLOPS
                )
            else:
                self.blackboard.log(
                    f"âŒ Model training failed: {model_name}",
                    level="ERROR",
                    agent=AgentType.MLOPS
                )

            # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
            report_file = self.reports_dir / f"training_{model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            report_file.write_text(json.dumps(training_report, indent=2))

            return training_report["success"], training_report

        except Exception as e:
            self.blackboard.log(
                f"âŒ Training exception: {str(e)}",
                level="ERROR",
                agent=AgentType.MLOPS
            )
            training_report["error"] = str(e)
            training_report["completed_at"] = datetime.now().isoformat()
            return False, training_report

    def _execute_training(
        self,
        training_script: str,
        params: Dict,
        dataset_path: Optional[str]
    ) -> Dict:
        """ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ"""
        try:
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’JSONå½¢å¼ã§æ¸¡ã™
            params_json = json.dumps(params)
            env = os.environ.copy()
            env["TRAINING_PARAMS"] = params_json

            if dataset_path:
                env["DATASET_PATH"] = dataset_path

            # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ
            result = subprocess.run(
                ["python", training_script],
                capture_output=True,
                text=True,
                timeout=3600,  # 1æ™‚é–“ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                env=env,
                cwd=self.repo_path
            )

            if result.returncode == 0:
                # æ¨™æº–å‡ºåŠ›ã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º
                metrics = self._extract_metrics_from_output(result.stdout)

                return {
                    "success": True,
                    "metrics": metrics,
                    "output": result.stdout,
                    "model_path": str(self.models_dir / "model.pkl")
                }
            else:
                return {
                    "success": False,
                    "error": result.stderr,
                    "output": result.stdout
                }

        except subprocess.TimeoutExpired:
            return {
                "success": False,
                "error": "Training timeout exceeded"
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }

    def _extract_metrics_from_output(self, output: str) -> Dict:
        """ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å‡ºåŠ›ã‹ã‚‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º"""
        metrics = {}

        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã§ä¸€èˆ¬çš„ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æŠ½å‡º
        patterns = {
            "accuracy": r"accuracy[:\s]+([0-9.]+)",
            "loss": r"loss[:\s]+([0-9.]+)",
            "f1_score": r"f1[:\s]+([0-9.]+)",
            "precision": r"precision[:\s]+([0-9.]+)",
            "recall": r"recall[:\s]+([0-9.]+)"
        }

        import re
        for metric_name, pattern in patterns.items():
            match = re.search(pattern, output, re.IGNORECASE)
            if match:
                try:
                    metrics[metric_name] = float(match.group(1))
                except:
                    pass

        return metrics

    def optimize_hyperparameters(
        self,
        model_name: str,
        training_script: str,
        param_space: Dict,
        n_trials: int = 10
    ) -> Tuple[bool, Dict]:
        """
        ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–

        Args:
            model_name: ãƒ¢ãƒ‡ãƒ«å
            training_script: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
            param_space: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ç©ºé–“
            n_trials: è©¦è¡Œå›æ•°

        Returns:
            (success, optimization_report)
        """
        self.blackboard.log(
            f"ğŸ”§ Optimizing hyperparameters for: {model_name} ({n_trials} trials)",
            level="INFO",
            agent=AgentType.MLOPS
        )

        optimization_report = {
            "model_name": model_name,
            "n_trials": n_trials,
            "started_at": datetime.now().isoformat(),
            "trials": [],
            "best_params": None,
            "best_score": None
        }

        try:
            import random

            best_score = -float('inf')
            best_params = None

            for trial in range(n_trials):
                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                sampled_params = self._sample_params(param_space)

                self.blackboard.log(
                    f"ğŸ”„ Trial {trial + 1}/{n_trials}: {sampled_params}",
                    level="INFO",
                    agent=AgentType.MLOPS
                )

                # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
                success, training_report = self.train_model(
                    f"{model_name}_trial_{trial}",
                    training_script,
                    sampled_params
                )

                # ã‚¹ã‚³ã‚¢å–å¾—
                score = training_report.get("metrics", {}).get("accuracy", 0)

                optimization_report["trials"].append({
                    "trial_number": trial + 1,
                    "params": sampled_params,
                    "score": score,
                    "success": success
                })

                # ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢æ›´æ–°
                if score > best_score:
                    best_score = score
                    best_params = sampled_params

            optimization_report["best_params"] = best_params
            optimization_report["best_score"] = best_score
            optimization_report["completed_at"] = datetime.now().isoformat()

            self.blackboard.log(
                f"âœ… Hyperparameter optimization complete. Best score: {best_score:.4f}",
                level="SUCCESS",
                agent=AgentType.MLOPS
            )

            # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
            report_file = self.reports_dir / f"optimization_{model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            report_file.write_text(json.dumps(optimization_report, indent=2))

            return True, optimization_report

        except Exception as e:
            self.blackboard.log(
                f"âŒ Hyperparameter optimization failed: {str(e)}",
                level="ERROR",
                agent=AgentType.MLOPS
            )
            optimization_report["error"] = str(e)
            optimization_report["completed_at"] = datetime.now().isoformat()
            return False, optimization_report

    def _sample_params(self, param_space: Dict) -> Dict:
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
        import random

        sampled = {}
        for param_name, space_def in param_space.items():
            space_type = space_def.get("type", "uniform")

            if space_type == "uniform":
                # é€£ç¶šå€¤
                low = space_def.get("low", 0)
                high = space_def.get("high", 1)
                sampled[param_name] = random.uniform(low, high)

            elif space_type == "int":
                # æ•´æ•°å€¤
                low = space_def.get("low", 0)
                high = space_def.get("high", 100)
                sampled[param_name] = random.randint(low, high)

            elif space_type == "choice":
                # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«
                choices = space_def.get("choices", [])
                sampled[param_name] = random.choice(choices)

            elif space_type == "log_uniform":
                # å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«
                low = space_def.get("low", 0.001)
                high = space_def.get("high", 1)
                sampled[param_name] = 10 ** random.uniform(
                    np.log10(low), np.log10(high)
                )

        return sampled

    def version_model(self, model_path: str, version: str, metadata: Optional[Dict] = None) -> Tuple[bool, str]:
        """
        ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°

        Args:
            model_path: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            version: ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·
            metadata: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿

        Returns:
            (success, versioned_path)
        """
        self.blackboard.log(
            f"ğŸ“¦ Versioning model: {model_path} -> v{version}",
            level="INFO",
            agent=AgentType.MLOPS
        )

        try:
            model_path = Path(model_path)
            if not model_path.exists():
                return False, f"Model file not found: {model_path}"

            # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            version_dir = self.models_dir / f"v{version}"
            version_dir.mkdir(parents=True, exist_ok=True)

            # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼
            versioned_model_path = version_dir / model_path.name
            shutil.copy2(model_path, versioned_model_path)

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
            if metadata is None:
                metadata = {}

            metadata.update({
                "version": version,
                "created_at": datetime.now().isoformat(),
                "original_path": str(model_path),
                "model_hash": self._compute_file_hash(model_path)
            })

            metadata_path = version_dir / "metadata.json"
            metadata_path.write_text(json.dumps(metadata, indent=2))

            self.blackboard.log(
                f"âœ… Model versioned: {versioned_model_path}",
                level="SUCCESS",
                agent=AgentType.MLOPS
            )

            return True, str(versioned_model_path)

        except Exception as e:
            self.blackboard.log(
                f"âŒ Model versioning failed: {str(e)}",
                level="ERROR",
                agent=AgentType.MLOPS
            )
            return False, str(e)

    def _compute_file_hash(self, file_path: Path) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—"""
        sha256_hash = hashlib.sha256()
        with open(file_path, "rb") as f:
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()

    def setup_ab_test(
        self,
        model_a: str,
        model_b: str,
        traffic_split: float = 0.5
    ) -> Tuple[bool, Dict]:
        """
        A/Bãƒ†ã‚¹ãƒˆã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

        Args:
            model_a: ãƒ¢ãƒ‡ãƒ«Aã®ãƒ‘ã‚¹
            model_b: ãƒ¢ãƒ‡ãƒ«Bã®ãƒ‘ã‚¹
            traffic_split: ãƒ¢ãƒ‡ãƒ«Aã¸ã®ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯å‰²åˆ (0-1)

        Returns:
            (success, ab_test_config)
        """
        self.blackboard.log(
            f"ğŸ”€ Setting up A/B test: {model_a} vs {model_b} (split: {traffic_split:.2f})",
            level="INFO",
            agent=AgentType.MLOPS
        )

        try:
            ab_test_config = {
                "test_id": f"abtest_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                "model_a": {
                    "path": model_a,
                    "traffic_weight": traffic_split
                },
                "model_b": {
                    "path": model_b,
                    "traffic_weight": 1 - traffic_split
                },
                "created_at": datetime.now().isoformat(),
                "status": "active",
                "metrics": {
                    "model_a": {"requests": 0, "latency_ms": [], "errors": 0},
                    "model_b": {"requests": 0, "latency_ms": [], "errors": 0}
                }
            }

            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            config_path = self.experiments_dir / f"{ab_test_config['test_id']}.json"
            config_path.write_text(json.dumps(ab_test_config, indent=2))

            self.blackboard.log(
                f"âœ… A/B test configured: {config_path}",
                level="SUCCESS",
                agent=AgentType.MLOPS
            )

            return True, ab_test_config

        except Exception as e:
            self.blackboard.log(
                f"âŒ A/B test setup failed: {str(e)}",
                level="ERROR",
                agent=AgentType.MLOPS
            )
            return False, {"error": str(e)}

    def analyze_ab_test(self, test_id: str) -> Dict:
        """
        A/Bãƒ†ã‚¹ãƒˆçµæœã‚’åˆ†æ

        Args:
            test_id: ãƒ†ã‚¹ãƒˆID

        Returns:
            åˆ†æãƒ¬ãƒãƒ¼ãƒˆ
        """
        self.blackboard.log(
            f"ğŸ“Š Analyzing A/B test: {test_id}",
            level="INFO",
            agent=AgentType.MLOPS
        )

        try:
            config_path = self.experiments_dir / f"{test_id}.json"
            if not config_path.exists():
                return {"error": f"Test config not found: {test_id}"}

            config = json.loads(config_path.read_text())
            metrics = config.get("metrics", {})

            model_a_metrics = metrics.get("model_a", {})
            model_b_metrics = metrics.get("model_b", {})

            # çµ±è¨ˆåˆ†æ
            analysis = {
                "test_id": test_id,
                "timestamp": datetime.now().isoformat(),
                "model_a": {
                    "requests": model_a_metrics.get("requests", 0),
                    "avg_latency_ms": self._calculate_avg(model_a_metrics.get("latency_ms", [])),
                    "error_rate": self._calculate_error_rate(
                        model_a_metrics.get("errors", 0),
                        model_a_metrics.get("requests", 0)
                    )
                },
                "model_b": {
                    "requests": model_b_metrics.get("requests", 0),
                    "avg_latency_ms": self._calculate_avg(model_b_metrics.get("latency_ms", [])),
                    "error_rate": self._calculate_error_rate(
                        model_b_metrics.get("errors", 0),
                        model_b_metrics.get("requests", 0)
                    )
                }
            }

            # å‹è€…åˆ¤å®š
            if analysis["model_a"]["error_rate"] < analysis["model_b"]["error_rate"]:
                analysis["winner"] = "model_a"
                analysis["reason"] = "Lower error rate"
            elif analysis["model_b"]["error_rate"] < analysis["model_a"]["error_rate"]:
                analysis["winner"] = "model_b"
                analysis["reason"] = "Lower error rate"
            elif analysis["model_a"]["avg_latency_ms"] < analysis["model_b"]["avg_latency_ms"]:
                analysis["winner"] = "model_a"
                analysis["reason"] = "Lower latency"
            elif analysis["model_b"]["avg_latency_ms"] < analysis["model_a"]["avg_latency_ms"]:
                analysis["winner"] = "model_b"
                analysis["reason"] = "Lower latency"
            else:
                analysis["winner"] = "tie"
                analysis["reason"] = "No significant difference"

            self.blackboard.log(
                f"âœ… A/B test analysis complete. Winner: {analysis['winner']}",
                level="SUCCESS",
                agent=AgentType.MLOPS
            )

            return analysis

        except Exception as e:
            self.blackboard.log(
                f"âŒ A/B test analysis failed: {str(e)}",
                level="ERROR",
                agent=AgentType.MLOPS
            )
            return {"error": str(e)}

    def _calculate_avg(self, values: List[float]) -> float:
        """å¹³å‡å€¤ã‚’è¨ˆç®—"""
        return sum(values) / len(values) if values else 0.0

    def _calculate_error_rate(self, errors: int, total: int) -> float:
        """ã‚¨ãƒ©ãƒ¼ç‡ã‚’è¨ˆç®—"""
        return (errors / total * 100) if total > 0 else 0.0

    def detect_model_drift(self, model_path: str, validation_data: Optional[str] = None) -> Dict:
        """
        ãƒ¢ãƒ‡ãƒ«ãƒ‰ãƒªãƒ•ãƒˆã‚’æ¤œçŸ¥

        Args:
            model_path: ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
            validation_data: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹

        Returns:
            ãƒ‰ãƒªãƒ•ãƒˆãƒ¬ãƒãƒ¼ãƒˆ
        """
        self.blackboard.log(
            f"ğŸ” Detecting model drift: {model_path}",
            level="INFO",
            agent=AgentType.MLOPS
        )

        try:
            # ãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—
            model_dir = Path(model_path).parent
            metadata_path = model_dir / "metadata.json"

            if not metadata_path.exists():
                return {"error": "Model metadata not found"}

            metadata = json.loads(metadata_path.read_text())
            baseline_metrics = metadata.get("metrics", {})

            # ç¾åœ¨ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡ï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰
            current_metrics = {
                "accuracy": 0.85,  # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
                "loss": 0.25
            }

            # ãƒ‰ãƒªãƒ•ãƒˆåˆ¤å®š
            drift_detected = False
            drift_details = []

            for metric_name, baseline_value in baseline_metrics.items():
                current_value = current_metrics.get(metric_name, 0)
                drift_percentage = abs(current_value - baseline_value) / baseline_value * 100

                if drift_percentage > 10:  # 10%ä»¥ä¸Šã®å¤‰åŒ–
                    drift_detected = True
                    drift_details.append({
                        "metric": metric_name,
                        "baseline": baseline_value,
                        "current": current_value,
                        "drift_percentage": drift_percentage
                    })

            drift_report = {
                "model_path": model_path,
                "timestamp": datetime.now().isoformat(),
                "drift_detected": drift_detected,
                "drift_details": drift_details,
                "baseline_metrics": baseline_metrics,
                "current_metrics": current_metrics
            }

            if drift_detected:
                self.blackboard.log(
                    f"âš ï¸ Model drift detected: {len(drift_details)} metrics drifted",
                    level="WARNING",
                    agent=AgentType.MLOPS
                )
            else:
                self.blackboard.log(
                    "âœ… No model drift detected",
                    level="SUCCESS",
                    agent=AgentType.MLOPS
                )

            return drift_report

        except Exception as e:
            self.blackboard.log(
                f"âŒ Model drift detection failed: {str(e)}",
                level="ERROR",
                agent=AgentType.MLOPS
            )
            return {"error": str(e)}

    def run_full_cycle(
        self,
        model_name: str,
        training_script: str,
        optimize: bool = False
    ) -> Dict:
        """
        å®Œå…¨ãªMLOpsã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œ

        ãƒ•ãƒ­ãƒ¼:
        1. ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
        2. (ã‚ªãƒ—ã‚·ãƒ§ãƒ³) ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
        3. ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°
        4. ãƒ‰ãƒªãƒ•ãƒˆæ¤œçŸ¥

        Returns:
            å®Ÿè¡Œãƒ¬ãƒãƒ¼ãƒˆ
        """
        report = {
            "started_at": datetime.now().isoformat(),
            "steps": [],
            "success": False
        }

        # Step 1: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
        if optimize:
            # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
            param_space = {
                "learning_rate": {"type": "uniform", "low": 0.0001, "high": 0.01},
                "batch_size": {"type": "choice", "choices": [16, 32, 64, 128]},
                "epochs": {"type": "int", "low": 10, "high": 100}
            }

            success, opt_report = self.optimize_hyperparameters(
                model_name,
                training_script,
                param_space,
                n_trials=5
            )

            report["steps"].append({
                "step": "optimize_hyperparameters",
                "success": success,
                "best_params": opt_report.get("best_params"),
                "best_score": opt_report.get("best_score")
            })

            # ãƒ™ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
            if success and opt_report.get("best_params"):
                success, train_report = self.train_model(
                    model_name,
                    training_script,
                    opt_report["best_params"]
                )
            else:
                success, train_report = self.train_model(model_name, training_script)
        else:
            # é€šå¸¸ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
            success, train_report = self.train_model(model_name, training_script)

        report["steps"].append({
            "step": "train_model",
            "success": success,
            "metrics": train_report.get("metrics", {})
        })

        if not success:
            report["message"] = "Training failed"
            return report

        # Step 2: ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°
        model_path = train_report.get("model_path")
        if model_path:
            version = datetime.now().strftime("%Y%m%d_%H%M%S")
            success, versioned_path = self.version_model(
                model_path,
                version,
                {"metrics": train_report.get("metrics", {})}
            )

            report["steps"].append({
                "step": "version_model",
                "success": success,
                "version": version,
                "path": versioned_path
            })

        # Step 3: ãƒ‰ãƒªãƒ•ãƒˆæ¤œçŸ¥
        if model_path:
            drift_report = self.detect_model_drift(model_path)
            report["steps"].append({
                "step": "detect_drift",
                "success": "error" not in drift_report,
                "drift_detected": drift_report.get("drift_detected", False)
            })

        report["completed_at"] = datetime.now().isoformat()
        report["success"] = True
        report["message"] = "Full MLOps cycle completed"

        return report


# ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³å®Ÿè¡Œç”¨
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="MLOps Agent")
    parser.add_argument("--repo", default=".", help="Repository path")
    parser.add_argument("--action", default="full",
                       choices=["train", "optimize", "version", "abtest", "drift", "full"],
                       help="Action to perform")
    parser.add_argument("--model-name", default="model", help="Model name")
    parser.add_argument("--script", help="Training script path")
    parser.add_argument("--optimize", action="store_true", help="Run hyperparameter optimization")

    args = parser.parse_args()

    agent = MLOpsAgent(args.repo)

    if args.action == "train":
        if not args.script:
            print("Error: --script required for training")
            sys.exit(1)

        success, report = agent.train_model(args.model_name, args.script)
        print(f"Training: {success}")
        print(json.dumps(report, indent=2))

    elif args.action == "optimize":
        if not args.script:
            print("Error: --script required for optimization")
            sys.exit(1)

        param_space = {
            "learning_rate": {"type": "uniform", "low": 0.0001, "high": 0.01},
            "batch_size": {"type": "choice", "choices": [16, 32, 64]}
        }

        success, report = agent.optimize_hyperparameters(
            args.model_name,
            args.script,
            param_space,
            n_trials=10
        )
        print(f"Optimization: {success}")
        print(json.dumps(report, indent=2))

    elif args.action == "version":
        success, path = agent.version_model("models/model.pkl", "1.0.0")
        print(f"Versioned: {success}")
        print(f"Path: {path}")

    elif args.action == "abtest":
        success, config = agent.setup_ab_test("models/v1/model.pkl", "models/v2/model.pkl")
        print(f"A/B test: {success}")
        print(json.dumps(config, indent=2))

    elif args.action == "drift":
        report = agent.detect_model_drift("models/v1/model.pkl")
        print(json.dumps(report, indent=2))

    elif args.action == "full":
        if not args.script:
            print("Error: --script required for full cycle")
            sys.exit(1)

        report = agent.run_full_cycle(
            args.model_name,
            args.script,
            optimize=args.optimize
        )
        print(json.dumps(report, indent=2))
